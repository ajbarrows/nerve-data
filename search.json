[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NERVE Lab Data Management",
    "section": "",
    "text": "Current path for UVM VACC users: /users/a/j/ajbarrow/scratch/data\nDatasets are all BIDS compliant (at least in structure), with one notable exception: .parquet files are included in addition to .tsv files where appropriate.\nAll data come from the NBCD Data Hub."
  },
  {
    "objectID": "index.html#hbcd",
    "href": "index.html#hbcd",
    "title": "NERVE Lab Data Management",
    "section": "HBCD",
    "text": "HBCD\n\nRelease Docs\nData Dictionary (DEAP)\nVolkow et al. (2024)\nDevelopmental Cognitive Neuroscience Special Issue\n\n\n1.0\nCitation:\n@misc{https://doi.org/10.82525/qv5x-qx11,\n  doi = {10.82525/QV5X-QX11},\n  url = {https://nbdc-datahub.org/hbcd-release-1-0},\n  author = {Chambers,  C and Nelson,  C and Dale,  A and Fair,  D and Smyser,  C and Acheson,  A and Bakhireva,  L and Bandoli,  G and Bell,  MA and Berry,  U and Bogdan,  R and Bosquet Enlow,  M and Coles,  C and Croff,  J and Cutting,  L and Dean,  D and DeMauro,  S and Engel,  S and Fox,  N and Gahagan,  S and Gao,  W and Garavan,  H and Georgieff,  M and Graham,  A and Grant,  E and Gregory,  K and Gurka,  K and Gurka,  M and Hays Grudo,  J and Hosig,  K and Howell,  B and Huang,  H and Johnson,  S and Jones Harden,  B and Kable,  J and Kaufman,  J and Levitt,  P and Lin,  W and McKelvey,  L and Merhar,  S and Morris,  A and Nagel,  B and Newman,  S and Newsom,  C and Norton,  E and Osmundson,  S and Ou,  X and Pekar,  J and Peralta-Carcelen,  M and Perez-Edgar,  K and Poehlmann-Tynan,  J and Potter,  A and Riggins,  T and Rogers,  C and Satin,  A and Scott,  L and Shenberger,  J and Shuffrey,  L and Smith,  B and Smith,  L and Stamilio,  D and Sullivan,  E and Thomason,  M and Vannest,  J and Volk,  H and Wakschlag,  L and Wilson,  S and Wisnowski,  J and Yerby,  L and Zgierska,  A and Zilverstand,  A},\n  title = {HBCD Study Data Release 1.0},\n  publisher = {Lasso Informatics US Inc},\n  year = {2025}\n}\nStructure:\nHBCD/1.0/\n|__ rawdata/ \n|   |__ phenotype/     # Tabulated Data (demographics, visit info, behavior, etc.)\n|   |   |__ par_visit_data.*\n|   |   |__ sed_basic_demographics.*\n|   |   |__ &lt;instrument_name&gt;.*\n|   |\n|   |__ sub-&lt;label&gt;/   # Raw File-Based Data (MRI, EEG, etc.)\n|   |   |__ sub-&lt;label&gt;_sessions.tsv\n|   |   |__ sub-&lt;label&gt;_sessions.json\n|   |   |__ ses-&lt;label&gt;/\n|   |       |__ anat/\n|   |       |__ dwi/\n|   |       |__ eeg/\n|   |       |__ fmap/\n|   |       |__ func/\n|   |       |__ motion/\n|   |       |__ mrs/\n|   |       |__ sub-&lt;label&gt;_ses-&lt;label&gt;_scans.tsv\n|   |       |__ sub-&lt;label&gt;_ses-&lt;label&gt;_scans.json\n|   |\n|   |__ dataset_description.json\n|   |__ participants.tsv\n|   |__ participants.json \n|\n|__ derivatives/        # Processed File-Based Data (MRI, EEG, etc.)\n    |__ bibsnet/\n    |__ hbcd_motion/\n    |__ made/\n    |__ mriqc/\n    |__ nibabies/\n    |__ osprey/\n    |__ qmri_postproc/\n    |__ qsiprep/\n    |__ qsirecon/\n    |__ symri/\n    |__ xcp_d/\n\n\nTotal Subjects: 602\n\n\n\n\n| session   |   anat |   dwi |   eeg |   fmap |   func |   motion |   mrs |\n|:----------|-------:|------:|------:|-------:|-------:|---------:|------:|\n| ses-V02   |    546 |   483 |     0 |    508 |    513 |      497 |   302 |\n| ses-V03   |     75 |    63 |    81 |     71 |     71 |       82 |    38 |"
  },
  {
    "objectID": "index.html#abcd",
    "href": "index.html#abcd",
    "title": "NERVE Lab Data Management",
    "section": "ABCD",
    "text": "ABCD\n\nABCD Wiki\nData Structure\nData Dictionary (DEAP)\n\n\n6.0\nCitation\n@misc{https://doi.org/10.82525/jy7n-g441,\n  doi = {10.82525/JY7N-G441},\n  url = {https://nbdc-datahub.org/abcd-release-6-0},\n  author = {Jernigan,  Terry L. and Brown,  Sandra A. and Dale,  Anders M. and Tapert,  Susan F. and Sowell,  Elizabeth R. and Herting,  Megan and Laird,  Angela and Gonzalez,  Raul and Squeglia,  Lindsay and Gray,  Kevin and Paulus,  Martin P. and Aupperle,  Robin and Feldstein Ewing,  Sarah W. and Nagel,  Bonnie J. and Fair,  Damien A. and Baker,  Fiona and M\\\"{u}ller Oehring,  Eva and Bookheimer,  Susan Y. and Dapretto,  Mirella and Jacobus,  Joanna and Wilson,  Sylia and Banich,  Marie T. and Cottler,  Linda B. and Nixon,  Sara Jo and Ernst,  Thomas M. and Chang,  Linda and Heitzeg,  Mary M. and Sripada,  Chandra and Luciana,  Monica M. and Friedman,  Naomi and Clark,  Duncan B. and Luna,  Beatriz and Foxe,  John and Freedman,  Edward and Yurgelun-Todd,  Deborah A. and Renshaw,  Perry F. and Potter,  Alexandra and Garavan,  Hugh P. and Lisdahl,  Krista and Larson,  Christine and Bjork,  James M. and Neale,  Michael C. and Heath,  Andrew C. and Barch,  Deanna M. and Madden,  Pamela A. and Casey,  Betty J. and Baskin-Sommers,  Arielle and Gee,  Dylan},\n  title = {ABCD Study(R) Data Release 6.0},\n  publisher = {Lasso Informatics US Inc},\n  year = {2025}\n}\nNote: We only maintain a subset of available ABCD data due to storage constraints. Please get in touch if there is information you’re missing.\nABCD/6.0/\n    dairc\n    └── rawdata\n        ├── phenotype\n        │   ├── &lt;table_name&gt;.json\n        │   ├── &lt;table_name&gt;.parquet\n        │   ├── &lt;table_name&gt;.tsv\n        │   └── ...\n        ├── sub-&lt;participant&gt;\n        │   ├── ses-&lt;event&gt;\n        │   └── ...\n        ├── ...\n        ├── dataset_description.json\n        ├── participants.json\n        ├── participants.tsv\n        ├── scans.json\n        ├── sessions.json\n        ├── task-&lt;experiment&gt;_beh.json\n        └── ...\n    └── concat\n        └── substance_use\n            └── tlfb\n\n\n\nTotal Subjects: 11868\n\n\n\n\n| session   |   beh |\n|:----------|------:|\n| ses-00A   | 11852 |\n| ses-01A   |  9009 |\n| ses-02A   | 10855 |\n| ses-03A   |  8482 |\n| ses-04A   |  9644 |\n| ses-05A   |  8615 |\n| ses-06A   |  5014 |"
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "Data Management and Analysis Guides",
    "section": "",
    "text": "A series of guides that might help some people (sometimes) who work with ABCD and HBCD data.",
    "crumbs": [
      "Guide",
      "Guides"
    ]
  },
  {
    "objectID": "guide.html#introduction",
    "href": "guide.html#introduction",
    "title": "Data Management and Analysis Guides",
    "section": "Introduction",
    "text": "Introduction\nThis website is meant to serve as an introduction to:\n\nloading and analyzing large neuroimaging datasets at UVM\ndata science project best practices\ndoing reproducible science.\n\nThis guide presumes the following:\n\nyou have an account with the Vermont Advanced Computing Center (VACC)\nyou are using one of the following operating systems:\n\nLinux (any distribution)\nmacOS\nWindows Subsystem for Linux (WSL)",
    "crumbs": [
      "Guide",
      "Guides"
    ]
  },
  {
    "objectID": "starting_projects.html",
    "href": "starting_projects.html",
    "title": "Starting a Project",
    "section": "",
    "text": "All successful data science projects begin with a semi-rigorous file structure1.\nRecommended reading:\n\ncookiecutter data science\nkedro\n\nNote: these are opinions. They’re best-practice-informed opinions, but still opinions.\n\n\n.\n├── data\n│   ├── processed\n│   └── raw\n├── LICENSE.md\n├── models\n├── notebooks\n├── README.md\n└── reports\n    └── figures\n/data contains (1) actual data files, or (2) symbolic links to data files (more on that later). /data/raw is data your code hasn’t touched. /data/processed are derivatives created within the current project.\n/models contains fitted/trained serialized model objects (e.g., Python .pkl or R .rds files)\n/notebooks contains exploratory and prototyping code written in Jupyter or R Markdown/Quarto notebooks. This is generally not where the analysis “actually happens.”\n/reports contains presentation materials you intend to share (e.g., LaTeX, Quarto, .pdf, .docx)\nREADME.md describes essential information about the project and its data repository\nLICENSE.md is a standardized file describing how your code may be used.\n\nRPython\n\n\n.\n├── {project-name}.Rproj\n├── R\n│   └── {load_data}.R\n├── renv\n├── renv.lock\n/{}.Rproj` file – basic collection of settings interpreted by RStudio. Not strictly necessary, but handy.\n/renv and renv.lock are environment files created by the package renv. More on virtual environments later.\n/R contains R scripts where the analysis “actually happens.”\n\n\n.\n├── environment.yml\n├── src\n│   └── {load_data}.py\n\n\n\n/environment.yml is a “frozen” file created by the library conda. More on virtual environments later.\n/src contains Python scripts where the analysis “actually happens.”",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Starting a Project"
    ]
  },
  {
    "objectID": "starting_projects.html#structure",
    "href": "starting_projects.html#structure",
    "title": "Starting a Project",
    "section": "",
    "text": "All successful data science projects begin with a semi-rigorous file structure1.\nRecommended reading:\n\ncookiecutter data science\nkedro\n\nNote: these are opinions. They’re best-practice-informed opinions, but still opinions.\n\n\n.\n├── data\n│   ├── processed\n│   └── raw\n├── LICENSE.md\n├── models\n├── notebooks\n├── README.md\n└── reports\n    └── figures\n/data contains (1) actual data files, or (2) symbolic links to data files (more on that later). /data/raw is data your code hasn’t touched. /data/processed are derivatives created within the current project.\n/models contains fitted/trained serialized model objects (e.g., Python .pkl or R .rds files)\n/notebooks contains exploratory and prototyping code written in Jupyter or R Markdown/Quarto notebooks. This is generally not where the analysis “actually happens.”\n/reports contains presentation materials you intend to share (e.g., LaTeX, Quarto, .pdf, .docx)\nREADME.md describes essential information about the project and its data repository\nLICENSE.md is a standardized file describing how your code may be used.\n\nRPython\n\n\n.\n├── {project-name}.Rproj\n├── R\n│   └── {load_data}.R\n├── renv\n├── renv.lock\n/{}.Rproj` file – basic collection of settings interpreted by RStudio. Not strictly necessary, but handy.\n/renv and renv.lock are environment files created by the package renv. More on virtual environments later.\n/R contains R scripts where the analysis “actually happens.”\n\n\n.\n├── environment.yml\n├── src\n│   └── {load_data}.py\n\n\n\n/environment.yml is a “frozen” file created by the library conda. More on virtual environments later.\n/src contains Python scripts where the analysis “actually happens.”",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Starting a Project"
    ]
  },
  {
    "objectID": "starting_projects.html#virtual-environment",
    "href": "starting_projects.html#virtual-environment",
    "title": "Starting a Project",
    "section": "Virtual Environment",
    "text": "Virtual Environment\n\nR",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Starting a Project"
    ]
  },
  {
    "objectID": "starting_projects.html#footnotes",
    "href": "starting_projects.html#footnotes",
    "title": "Starting a Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany doomed projects also begin with semi-rigorous data structures.↩︎",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Starting a Project"
    ]
  },
  {
    "objectID": "using_git.html",
    "href": "using_git.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Note\n\n\n\nMy LLM friend Claude and I made this together.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#what-is-git",
    "href": "using_git.html#what-is-git",
    "title": "Git and GitHub",
    "section": "What is Git?",
    "text": "What is Git?\nGit is a version control system that tracks changes in your files. It works locally on your computer and creates snapshots (called commits) of your work. Think of it like “track changes” for code and data projects, but much more powerful.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#what-is-github",
    "href": "using_git.html#what-is-github",
    "title": "Git and GitHub",
    "section": "What is GitHub?",
    "text": "What is GitHub?\nGitHub is a remote hosting platform for Git repositories. It provides:\n\nBackup in the cloud\nTools to share and collaborate\nA portfolio of your work\nProject management features\n\nKey distinction: Git runs locally on your machine, while GitHub is the remote service where you store your repositories.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#git-basics",
    "href": "using_git.html#git-basics",
    "title": "Git and GitHub",
    "section": "Git Basics",
    "text": "Git Basics\n\nCore Concepts\nRepository (repo): A project folder that Git is tracking. Contains all your files plus a hidden .git directory with the version history.\nCommit: A snapshot of your work at a specific point in time. Each commit has a unique ID and a message describing what changed.\nBranch: A separate line of development. We’ll keep things simple with just the main branch for now.\n\n\nWhat Git Tracks\nGit is great for tracking certain types of files:\n✅ DO track:\n\nCode files (.py, .R, .jl)\nNotebooks (.ipynb, .qmd, .Rmd)\nDocumentation (README.md, notes)\nConfiguration files (requirements.txt, config.yaml)\n\n❌ DON’T track:\n\nLarge datasets (use data storage solutions instead)\nAPI keys and credentials (use environment variables)\nGenerated outputs (plots, HTML reports)\nBinary files that change frequently\n\n\n\nBasic Workflow\nThe fundamental Git workflow follows these steps:\n\nModify files in your project\nStage changes with git add\nCommit changes with git commit\nRepeat!\n\nThis pattern becomes second nature once you practice it a few times.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#essential-commands",
    "href": "using_git.html#essential-commands",
    "title": "Git and GitHub",
    "section": "Essential Commands",
    "text": "Essential Commands\nHere are the 5 commands you need to get started with Git:\n# Start tracking a project\ngit init\n\n# Stage changes (prepare files for commit)\ngit add filename.py\n\n# Save a snapshot with a message\ngit commit -m \"Clear message about what changed\"\n\n# Check what's changed\ngit status\n\n# View commit history\ngit log\n\nExample: Creating Your First Repository\n# Create and navigate to a new project\ngit init my-analysis\ncd my-analysis\n\n# Create a simple analysis file\n# (create your notebook or script here)\n\n# Stage and commit\ngit add analysis.ipynb\ngit commit -m \"Initial analysis of dataset X\"",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#github-for-collaboration",
    "href": "using_git.html#github-for-collaboration",
    "title": "Git and GitHub",
    "section": "GitHub for Collaboration",
    "text": "GitHub for Collaboration\n\nConnecting Local and Remote\nOnce you have a local Git repository, you can connect it to GitHub to back it up and share it with others.\nTwo new commands:\n# Send your commits to GitHub\ngit push\n\n# Get the latest changes from GitHub\ngit pull\n\n\nTypical Workflow with GitHub\n\nCreate a repository on GitHub\nConnect your local repository to GitHub\nPush your work to the remote\nCollaborate with others who can clone or contribute\nPull their changes to stay up to date\n\n\n\nPushing to GitHub\n# Connect to GitHub (one time setup)\ngit remote add origin https://github.com/username/repo.git\n\n# Push your commits\ngit push -u origin main\nAfter pushing, anyone with access can view and use your analysis through GitHub’s web interface.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#data-science-specific-considerations",
    "href": "using_git.html#data-science-specific-considerations",
    "title": "Git and GitHub",
    "section": "Data Science Specific Considerations",
    "text": "Data Science Specific Considerations\n\nWhat to Commit\nFor data science projects, make sure to include:\n\nAnalysis scripts (.py, .R, .jl)\nJupyter notebooks or Quarto documents\nDependency files (requirements.txt, renv.lock, environment.yml)\nREADME.md with project description and setup instructions\nDocumentation and notes\n\n\n\nWhat NOT to Commit\nUse a .gitignore file to exclude:\n\nData files (.csv, .parquet, .xlsx) - usually too large for Git\nCredentials (.env files, API keys, passwords)\nOutput files (plots, HTML reports, model files)\n\n\n\nThe .gitignore File\nCreate a .gitignore file in your repository root:\n# Data\n*.csv\n*.parquet\n*.xlsx\ndata/\nraw_data/\n\n# Credentials\n.env\nsecrets.json\n.Renviron\n\n# Outputs\n*.html\n*.png\n*.pdf\nfigures/\noutput/\n\n# Python\n__pycache__/\nvenv/\n*.pyc\n\n# R\n.Rproj.user/\n.Rhistory\n.RData\n\n# Jupyter\n.ipynb_checkpoints/\n\n\nWorking with Jupyter Notebooks\nJupyter notebooks can be tricky with version control because they contain outputs and metadata that change frequently.\nBest practices:\n\nClear outputs before committing: Cell → All Output → Clear\nUse tools like nbstripout to automatically strip outputs\nConsider using Quarto (.qmd) files instead - they’re plain text and version control friendly\nReview diffs carefully on GitHub to see what actually changed",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#hands-on-practice",
    "href": "using_git.html#hands-on-practice",
    "title": "Git and GitHub",
    "section": "Hands-On Practice",
    "text": "Hands-On Practice\nTo solidify these concepts, try this exercise:\n\nCreate a new directory for a simple analysis project\nInitialize Git with git init\nCreate a basic notebook or script with some analysis\nMake your first commit\nMake a change to your code and commit again\nCreate a repository on GitHub\nPush your local work to GitHub\n\nBonus: Add a .gitignore file appropriate for your project.",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#best-practices",
    "href": "using_git.html#best-practices",
    "title": "Git and GitHub",
    "section": "Best Practices",
    "text": "Best Practices\n\nCommit Messages\nWrite clear, descriptive commit messages that explain what changed and why.\nGood examples:\n\n\"Add data cleaning function for missing values\"\n\"Fix bug in correlation calculation\"\n\"Update visualization to use seaborn style\"\n\nBad examples:\n\n\"updates\"\n\"fixed stuff\"\n\"asdfasdf\"\n\n\n\nCommit Frequency\n\nCommit early, commit often\n\nMake small, focused commits rather than large ones that change many things. This makes it easier to:\n\nUnderstand what changed\nFind bugs by reviewing specific commits\nRevert changes if something breaks\n\n\n\nBranching (Advanced)\nAs you get more comfortable, explore branches for:\n\nTesting new features without breaking your main analysis\nCollaborating with others on different aspects\nKeeping stable and experimental work separate",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#resources-for-learning-more",
    "href": "using_git.html#resources-for-learning-more",
    "title": "Git and GitHub",
    "section": "Resources for Learning More",
    "text": "Resources for Learning More\n\nGit Documentation - Official Git documentation\nGitHub Skills - Interactive tutorials",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "using_git.html#key-takeaways",
    "href": "using_git.html#key-takeaways",
    "title": "Git and GitHub",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nVersion control is essential for reproducible data science\nStart simple with the basic commands: init, add, commit, push, pull\nCommit frequently with clear messages",
    "crumbs": [
      "Guide",
      "Reproducible Analysis",
      "Git and GitHub"
    ]
  },
  {
    "objectID": "vacc.html",
    "href": "vacc.html",
    "title": "High-Performance Computing",
    "section": "",
    "text": "Nodes (computers)\nEach node is, itself, very, very powerful.\nThere are lots of nodes (72 at UVM), and they’re linked together into a cluster\nEach of these nodes is, itself, very powerful\n\nUVM’s HPC cluster is the Vermont Advanced Computing Center (VACC).\n\n\n\nClusters typically run Linux\nThey typically don’t have graphic user interfaces (GUIs)\nThey involve login nodes to which you connect directly, and compute nodes where the work gets done\nThey often have some kind of scheduler. At UVM, we use SLURM."
  },
  {
    "objectID": "vacc.html#what-is-high-performance-computing-hpc",
    "href": "vacc.html#what-is-high-performance-computing-hpc",
    "title": "High-Performance Computing",
    "section": "",
    "text": "Nodes (computers)\nEach node is, itself, very, very powerful.\nThere are lots of nodes (72 at UVM), and they’re linked together into a cluster\nEach of these nodes is, itself, very powerful\n\nUVM’s HPC cluster is the Vermont Advanced Computing Center (VACC).\n\n\n\nClusters typically run Linux\nThey typically don’t have graphic user interfaces (GUIs)\nThey involve login nodes to which you connect directly, and compute nodes where the work gets done\nThey often have some kind of scheduler. At UVM, we use SLURM."
  },
  {
    "objectID": "vacc.html#using-the-vacc",
    "href": "vacc.html#using-the-vacc",
    "title": "High-Performance Computing",
    "section": "Using the VACC",
    "text": "Using the VACC\n\nPrerequisites:\n\nYou have a VACC account.\nYou have access to a terminal (e.g., through macOS, Linux, or Windows Subsystem for Linux)\n\nThere are several was to interface with the VACC:\n\ndirect access to a login node’s shell through SSH\nusing Open OnDemand, a browser-based interface which provides\n\nfile management\njob management and accounting\na command-line interface\ninteractive apps (e.g., Jupyter Notebook, RStudio Server, Matlab)\n\nusing “remote development” features of VSCode or a similar IDE (this is actually just 1. but different)\n\n\n\nCommand-line shell and SSH"
  },
  {
    "objectID": "access_data.html",
    "href": "access_data.html",
    "title": "UVM-Hosted Data",
    "section": "",
    "text": "Note\n\n\n\nGuiding philosophy: share data by sharing code to encourage best practices.\n\n\nGoal: Agree on a flexible, convenient way to produce collaborative, shareable data science projects.\n\n\n\nan account with the Vermont Advanced Computing Center (VACC)\none of the following operating systems:\n\nLinux (any distribution)\nmacOS\nWindows Subsystem for Linux (WSL)",
    "crumbs": [
      "Guide",
      "Accessing Data",
      "UVM-Hosted Data"
    ]
  },
  {
    "objectID": "access_data.html#overview",
    "href": "access_data.html#overview",
    "title": "UVM-Hosted Data",
    "section": "",
    "text": "Note\n\n\n\nGuiding philosophy: share data by sharing code to encourage best practices.\n\n\nGoal: Agree on a flexible, convenient way to produce collaborative, shareable data science projects.\n\n\n\nan account with the Vermont Advanced Computing Center (VACC)\none of the following operating systems:\n\nLinux (any distribution)\nmacOS\nWindows Subsystem for Linux (WSL)",
    "crumbs": [
      "Guide",
      "Accessing Data",
      "UVM-Hosted Data"
    ]
  },
  {
    "objectID": "access_data.html#accessing-data-stored-on-the-vacc",
    "href": "access_data.html#accessing-data-stored-on-the-vacc",
    "title": "UVM-Hosted Data",
    "section": "Accessing data stored on the VACC",
    "text": "Accessing data stored on the VACC\n\nUVM’s high-performance computing (HPC) cluster\n\n“cluster” refers to multiple compute ‘nodes’ working in concert\n\nAccessible primarily though SSH\n\nbut also through a GUI (Open OnDemand)\n\n\nOnce you have a VACC account, you’ll want to familiarize yourself with the documentation. This documentation incldues guides for\n\nLinux basics\nconnecting to the VACC’s “login” nodes\nSLURM – the VACC’s job submission manager\nlanguage-specific instructions\nmuch more…\n\nI’m going to assume you’ve reviewed these guides and have successfully connected to a VACC login node using SSH. I have an SSH “alias” set up to make things easier:\n\nYou can now navigate to where our in-house data sets are maintained:\ncd /users/a/j/ajbarrow/scratch/data\nUsing basic Linux commands, we can explore the file directory:",
    "crumbs": [
      "Guide",
      "Accessing Data",
      "UVM-Hosted Data"
    ]
  },
  {
    "objectID": "access_data.html#getting-data-into-a-your-data-directory",
    "href": "access_data.html#getting-data-into-a-your-data-directory",
    "title": "UVM-Hosted Data",
    "section": "Getting data into a your /data directory",
    "text": "Getting data into a your /data directory\n\n\n\n\n\n\nNote\n\n\n\nI assume you are using a rigorous project structure including a /data directory and /data/raw and /data/processed subdirectories. See Starting a Project for more information.\n\n\n\nWorking directly on the VACC\nIf you plan to do your data cleaning, exploration, analysis, and reporting entirely on the VACC (more on this later), you can simply create a symbolic link between the /data directory in your project and the shared NERVE Lab data repository. A symbolic link is just like a Windows or macOS shortcut, but it’s created using the terminal:\nln -s /path/to/file /path/to/symlink\nTo be concrete (assuming your project is caled my_project):\ncd my_project\nln -s /users/a/j/ajbarrow/scratch/data/ABCD data/raw\nThis will create a link between the repository that holds /ABCD and your project’s /data/raw folder. This means that you will access data from your local /data/raw/ folder – as will anyone who wants to reproduce your results!\n\n\nWorking on your local computer\nIf you don’t plan to work on the VACC directly (or if you will prototype your code on a local machine and possibly run analyses on the VACC later), you will need to copy data to your local machine 1. There are several methods for doing this. In any case, I strongly recommend that you maintain the original file structure.\n\nrsync (recommended)\nLinux and macOS have a built in tool called “remote sync” (rsync) which synchronizes file trees, often between “local” (e.g., your laptop) and “remote” (e.g., the VACC) resources. rsync commands take the form\nrsync [option] source destination\nI encourage you to become familiar with the various options used in rsync, but I will suggest a common implementation using teh -a (“archive”) flag which contains a number of common options (e.g., ensures all subdirectories are copied, preserves symlinks, and preserves ownership and permissions), the -v (“verbose”) flag that prints all errors and warnings, and the -P flag, which shows an interactive progress bar.\nFrom my local machine:\ncd my_project\nrsync -avP \\\\ \n    vacc:/users/a/j/ajbarrow/scratch/data/ABCD/6.0/dairc/rawdata/phenotype data/raw\nNote The \\\\ indicates a multi-line command. This is not necessary.\nOther note You will likely have to replace vacc with your_netid@login.vacc.uvm.edu.\n\n\nOpen OnDemand (GUI-based alternative)\nIn order to use Open OnDemand, you need to be on campus or connected to UVM’s VPN.",
    "crumbs": [
      "Guide",
      "Accessing Data",
      "UVM-Hosted Data"
    ]
  },
  {
    "objectID": "access_data.html#footnotes",
    "href": "access_data.html#footnotes",
    "title": "UVM-Hosted Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease do not interpret this section as a recommendation that you copy data from the VACC to your local machine.↩︎",
    "crumbs": [
      "Guide",
      "Accessing Data",
      "UVM-Hosted Data"
    ]
  }
]